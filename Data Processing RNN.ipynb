{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    data = f.read() #Reading in Sonnets (not setting lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the LSTM on data where sonnets are mildly processed\n",
    "#Each sonnet is stripped by line, and the indices are removed, and put back together again\n",
    "sonnets = data.split('\\n\\n')\n",
    "sonnet_lens = [len(sonnet) for sonnet in sonnets] #Splitting each sonnet up\n",
    "sonnets = [sonnet.strip() for sonnet in sonnets] #Removing whitespace\n",
    "\n",
    "lines = [sonnet.split('\\n') for sonnet in sonnets] #splitting up the sonnets into lines\n",
    "lines = [line[1:] for line in lines] #Removing index of poem\n",
    "lines = [[line.strip() for line in sonnet] for sonnet in lines] #removing whitespace from each line\n",
    "\n",
    "fulltext = \"\" \n",
    "for sonnet in lines:\n",
    "    for line in sonnet: #Putting all the lines back together, with a space between each poem\n",
    "        fulltext += line + \"\\n\"\n",
    "    fulltext += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 62\n",
      "['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '’']\n",
      "Size of training sequences: (18758, 40, 62)\n",
      "Size of training targets: (18758, 62)\n"
     ]
    }
   ],
   "source": [
    "#Formulation of data using direct dataset\n",
    "seqarray = [] #array of sequences of 40-length characters from fulltext\n",
    "nextchar = [] #array of chars following each 40-length sequence\n",
    "seqlength = 40\n",
    "step = 5\n",
    "for i in range(0, len(fulltext) - seqlength, step):\n",
    "    seqarray.append(fulltext[i:i + seqlength]) #adding sequence of 40 characters, every 20 characters\n",
    "    nextchar.append(fulltext[i + seqlength])\n",
    "    \n",
    "chars = sorted(list(set(fulltext))) #Getting all unique chars in data\n",
    "chars.append(\"’\")\n",
    "print(\"Number of unique characters:\", len(chars))\n",
    "print(chars)\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) # Dictionary mapping unique character to integer indices    \n",
    "\n",
    "# we can now 1-hot encode each character in our dataset, based on our dictionary we made \n",
    "x = np.zeros((len(seqarray), seqlength, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(seqarray), len(chars)), dtype=np.bool)\n",
    "for i, sequence in enumerate(seqarray):\n",
    "    for j, char in enumerate(sequence):\n",
    "        x[i, j, char_indices[char]] = 1 #encoding our X and Y, our data and target\n",
    "    y[i, char_indices[nextchar[i]]] = 1\n",
    "\n",
    "print(\"Size of training sequences:\", x.shape)\n",
    "print(\"Size of training targets:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 200)               210400    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 62)                12462     \n",
      "=================================================================\n",
      "Total params: 222,862\n",
      "Trainable params: 222,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Creating our Keras Model, with and LSTM layer and a dense softmax layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(seqlength, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 20s 1ms/step - loss: 3.3763\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 17s 921us/step - loss: 3.0988\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 17s 911us/step - loss: 3.0858\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 17s 927us/step - loss: 3.0731\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 17s 925us/step - loss: 3.0599\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 17s 912us/step - loss: 3.0407\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 17s 916us/step - loss: 3.0142\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 17s 912us/step - loss: 2.9774\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 17s 922us/step - loss: 2.9295\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 17s 912us/step - loss: 2.8715\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 949us/step - loss: 2.8065\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 951us/step - loss: 2.7350\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 18s 963us/step - loss: 2.6746\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 18s 946us/step - loss: 2.6234\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 18s 950us/step - loss: 2.5747\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 18s 946us/step - loss: 2.5313\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 18s 949us/step - loss: 2.4949\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 17s 926us/step - loss: 2.4629\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 18s 951us/step - loss: 2.4295\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 18s 940us/step - loss: 2.4071\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 946us/step - loss: 2.3792\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 946us/step - loss: 2.3589\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 18s 940us/step - loss: 2.3363\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 18s 956us/step - loss: 2.3177\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 18s 946us/step - loss: 2.2985\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 18s 955us/step - loss: 2.2796\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 18s 962us/step - loss: 2.2624\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 18s 953us/step - loss: 2.2397\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 18s 964us/step - loss: 2.2226\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 18s 951us/step - loss: 2.2130\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 960us/step - loss: 2.1941\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 969us/step - loss: 2.1767\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 18s 965us/step - loss: 2.1655\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 18s 968us/step - loss: 2.1541\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 18s 954us/step - loss: 2.1437\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 18s 974us/step - loss: 2.1322\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 18s 965us/step - loss: 2.1204\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 18s 959us/step - loss: 2.1099\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 18s 981us/step - loss: 2.0992\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 18s 965us/step - loss: 2.0892\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 966us/step - loss: 2.0770\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 966us/step - loss: 2.0722\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 18s 964us/step - loss: 2.0588\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 18s 954us/step - loss: 2.0511\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 18s 966us/step - loss: 2.0412\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 19s 991us/step - loss: 2.0312\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 2.0254\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 18s 958us/step - loss: 2.0149\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 18s 968us/step - loss: 2.0085\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 18s 960us/step - loss: 1.9995\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 970us/step - loss: 1.9934\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 19s 990us/step - loss: 1.9828\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 18s 984us/step - loss: 1.9747\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.9667\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 19s 991us/step - loss: 1.9614\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 19s 992us/step - loss: 1.9531\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 19s 999us/step - loss: 1.9450\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 18s 984us/step - loss: 1.9372\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.9305\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.9221\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 985us/step - loss: 1.9152\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.9072\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.9019\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 19s 989us/step - loss: 1.8930\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.8863\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 19s 990us/step - loss: 1.8794\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 19s 996us/step - loss: 1.8716\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.8661\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 19s 987us/step - loss: 1.8603\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 19s 993us/step - loss: 1.8513\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 19s 1ms/step - loss: 1.8471\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 986us/step - loss: 1.8389\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 19s 991us/step - loss: 1.8335\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 18s 985us/step - loss: 1.8237\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 19s 989us/step - loss: 1.8169\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 19s 996us/step - loss: 1.8096\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 19s 988us/step - loss: 1.8006\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 19s 995us/step - loss: 1.7977\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 18s 947us/step - loss: 1.7887\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 18s 984us/step - loss: 1.7809\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 19s 991us/step - loss: 1.7752\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 983us/step - loss: 1.7696\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 18s 986us/step - loss: 1.7616\n",
      "Epoch 4/10\n",
      "18758/18758 [==============================] - 19s 987us/step - loss: 1.7587\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 18s 984us/step - loss: 1.7469\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 19s 990us/step - loss: 1.7419\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 18s 965us/step - loss: 1.7315\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 19s 994us/step - loss: 1.7264\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 19s 990us/step - loss: 1.7201\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 19s 989us/step - loss: 1.7108\n",
      "Epoch 1/10\n",
      "18758/18758 [==============================] - 18s 968us/step - loss: 1.7026\n",
      "Epoch 2/10\n",
      "18758/18758 [==============================] - 18s 983us/step - loss: 1.6941\n",
      "Epoch 3/10\n",
      "18758/18758 [==============================] - 19s 994us/step - loss: 1.6890\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18758/18758 [==============================] - 18s 941us/step - loss: 1.6825\n",
      "Epoch 5/10\n",
      "18758/18758 [==============================] - 18s 951us/step - loss: 1.6742\n",
      "Epoch 6/10\n",
      "18758/18758 [==============================] - 18s 960us/step - loss: 1.6619\n",
      "Epoch 7/10\n",
      "18758/18758 [==============================] - 18s 939us/step - loss: 1.6561\n",
      "Epoch 8/10\n",
      "18758/18758 [==============================] - 18s 953us/step - loss: 1.6497\n",
      "Epoch 9/10\n",
      "18758/18758 [==============================] - 18s 950us/step - loss: 1.6424\n",
      "Epoch 10/10\n",
      "18758/18758 [==============================] - 18s 941us/step - loss: 1.6302\n",
      "[3.376349994503037, 2.806515699511259, 2.3791744040964966, 2.194111976202572, 2.0770246880257077, 1.9934385002059765, 1.915225080086842, 1.8471019242742521, 1.7752014348623608, 1.7026198313985885]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for i in range(10):\n",
    "    history = model.fit(x, y, batch_size=128, epochs=10)\n",
    "    loss.append(history.history['loss'][0])\n",
    "print(loss)\n",
    "\n",
    "model.save('LSTMShakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextchar(preds, temperature=1.0):\n",
    "    #We reweight the model using temperature\n",
    "    #We then sample from the prediction from the model\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature #shifting each prediciton by temperature\n",
    "    exp_preds = np.exp(preds) \n",
    "    preds = exp_preds / np.sum(exp_preds) #reweighting\n",
    "    probas = np.random.multinomial(1, preds, 1) #chooses 1 element from reweighted probabilities \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makesonnet(temp):\n",
    "    #Given a temperature, generate a new sonnet.\n",
    "    seed = \"shall i compare thee to a summer’s day?\\n\"\n",
    "    sonnet = \"\"    \n",
    "    for i in range(600):\n",
    "        # Vectorize generated text\n",
    "        sampled = np.zeros((1, seqlength, len(chars)))\n",
    "        for j, char in enumerate(seed):\n",
    "            sampled[0, j, char_indices[char]] = 1.\n",
    "\n",
    "        # Predict next character\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        pred_idx = nextchar(preds, temperature=temp)\n",
    "        next_char = chars[pred_idx]\n",
    "\n",
    "        # Append predicted character to new sonnet and seed text\n",
    "        sonnet += next_char\n",
    "        seed += next_char\n",
    "        #Remove first character of our seed text\n",
    "        seed = seed[1:]\n",
    "\n",
    "    return sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the wild to love the will ded my deat,\n",
      "And me hou to the thing the stret sun to thee,\n",
      "The I be my sure the peath the free do her,\n",
      "I thing a to the to thou dost do doth the sight,\n",
      "So love wo then be and the resto sull,\n",
      "So art my the be of thee to the tree des,\n",
      "The eye the bet be the arth the deare dee.\n",
      "\n",
      "Whe an the panter with the wine of me dot,\n",
      "And to the peate the cen the least of sell,\n",
      "I mants the reching the price the store dees,\n",
      "The beath the canker the well dese doth dost do free,\n",
      "Whe thy sell by under the for dest the ser,\n",
      "So sine in the trough doth live of ment,\n",
      "And the heart the wi\n"
     ]
    }
   ],
   "source": [
    "print(makesonnet(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The anow fresen tree wrat he line ould fore,\n",
      "When in thy withtr sem not ave ig on my deast\n",
      "\n",
      "Yor I sane mo no than to carper will warts,\n",
      "And right he restigh purne and were to wair thee stor,\n",
      "The but ne stern weave hor he wemy not,\n",
      "By fould most rost retire of hear her cenvys.\n",
      "elige co sulligh, I deir muse semes.\n",
      "Whele to forgpring so doth love praks of,\n",
      "Sears well beausit, an the with the art,\n",
      "O ale thou be buthed be bush and beturn wingreent:\n",
      "What ho hame selise do horg ma th un in myne dsey I hasp,\n",
      "And time the still whthe ho not of love by,\n",
      "Whingen I be sand and wat ot 'es bligen,\n",
      "And thou \n"
     ]
    }
   ],
   "source": [
    "print(makesonnet(0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mink pued tive en wimur gruss as solfer,\n",
      "Is anksulp)rpaps, and wij, if sestraF.\n",
      "\n",
      "Sonciken tealy,'rlicbed my nservilG?\n",
      "Ay daetTotlunose:med harz pulinheWh waithes cromeldH:;nl\n",
      "chal lath tBou eay'A bionftGep, peacinetARd is ly,\n",
      "Uly pie,\n",
      "Arsel, my ereat eath pilsed in thee lost,\n",
      "ShildavH ich ir tiughid, non igry heaPI:\n",
      "Whem contion weat)noteI of brose des),\n",
      "Peep foublfiegoqk werks thoutgrigk inottinvoSt:\n",
      "WinnPiwe-ec ' wrre sired Wgots\n",
      "Men baty arm gillce, whoulto domevfrlle bib,\n",
      "Then vose grae ba art is me,\n",
      "Ewaing nooly thee def'rded grofsl llowai:es'q,\n",
      "S\n",
      "rim geveed ewfly drightq'eul aflimgn.:\n",
      "Yr\n"
     ]
    }
   ],
   "source": [
    "print(makesonnet(1.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
