{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    data = f.read() #Reading in Sonnets (not setting lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the LSTM on data where sonnets are mildly processed\n",
    "#Each sonnet is stripped by line, and the indices are removed, and put back together again\n",
    "sonnets = data.split('\\n\\n')\n",
    "sonnet_lens = [len(sonnet) for sonnet in sonnets] #Splitting each sonnet up\n",
    "sonnets = [sonnet.strip() for sonnet in sonnets] #Removing whitespace\n",
    "\n",
    "lines = [sonnet.split('\\n') for sonnet in sonnets] #splitting up the sonnets into lines\n",
    "lines = [line[1:] for line in lines] #Removing index of poem\n",
    "lines = [[line.strip() for line in sonnet] for sonnet in lines] #removing whitespace from each line\n",
    "\n",
    "fulltext = \"\" \n",
    "for sonnet in lines:\n",
    "    for line in sonnet: #Putting all the lines back together, with a space between each poem\n",
    "        fulltext += line + \"\\n\"\n",
    "    fulltext += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 61\n",
      "Size of training sequences: (18758, 40, 61)\n",
      "Size of training targets: (18758, 61)\n"
     ]
    }
   ],
   "source": [
    "#Formulation of data using direct dataset\n",
    "seqarray = [] #array of sequences of 40-length characters from fulltext\n",
    "nextchar = [] #array of chars following each 40-length sequence\n",
    "seqlength = 40\n",
    "step = 5\n",
    "for i in range(0, len(fulltext) - seqlength, step):\n",
    "    seqarray.append(fulltext[i:i + seqlength]) #adding sequence of 40 characters, every 20 characters\n",
    "    nextchar.append(fulltext[i + seqlength])\n",
    "    \n",
    "chars = sorted(list(set(fulltext))) #Getting all unique chars in data\n",
    "print(\"Number of unique characters:\", len(chars))\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) # Dictionary mapping unique character to integer indices    \n",
    "\n",
    "# we can now 1-hot encode each character in our dataset, based on our dictionary we made \n",
    "x = np.zeros((len(seqarray), seqlength, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(seqarray), len(chars)), dtype=np.bool)\n",
    "for i, sequence in enumerate(seqarray):\n",
    "    for j, char in enumerate(sequence):\n",
    "        x[i, j, char_indices[char]] = 1 #encoding our X and Y, our data and target\n",
    "    y[i, char_indices[nextchar[i]]] = 1\n",
    "\n",
    "print(\"Size of training sequences:\", x.shape)\n",
    "print(\"Size of training targets:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               97280     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 61)                7869      \n",
      "=================================================================\n",
      "Total params: 105,149\n",
      "Trainable params: 105,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Creating our Keras Model, with and LSTM layer and a dense softmax layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlength, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 12s 663us/step - loss: 3.4231\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 12s 630us/step - loss: 3.0986\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 12s 627us/step - loss: 3.0836\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 16s 863us/step - loss: 3.0711\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 12s 650us/step - loss: 3.0574\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 11s 592us/step - loss: 3.0333\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 12s 642us/step - loss: 3.0030\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 14s 749us/step - loss: 2.9627\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 13s 695us/step - loss: 2.9154\n",
      "Epoch 1/1\n",
      "18758/18758 [==============================] - 12s 647us/step - loss: 2.8634\n",
      "[3.4231271635991627, 3.0986479029547165, 3.0835807940410596, 3.0711490453363597, 3.057409001510145, 3.03330727937265, 3.002952538951016, 2.962662413120524, 2.91543468024281, 2.8633807093645416]\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for i in range(10):\n",
    "    history = model.fit(x, y, batch_size=128, epochs=1)\n",
    "    loss.append(history.history['loss'][0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextchar(preds, temperature=1.0):\n",
    "    #We reweight the model using temperature predicted probabilities and draw sample from newly created probability distribution.\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
